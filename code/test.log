2023-07-11 21:43:31.014541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-11 21:43:31.905622: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
07/11/2023 21:43:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
07/11/2023 21:43:36 - INFO - __main__ -   Training/evaluation parameters Namespace(train_data_file='../dataset/train.jsonl', output_dir='./saved_models', eval_data_file='../dataset/valid.jsonl', test_data_file='../dataset/test.jsonl', model_type='roberta', model_name_or_path='microsoft/codebert-base', mlm=False, mlm_probability=0.15, config_name='', tokenizer_name='microsoft/codebert-base', cache_dir='', block_size=400, do_train=False, do_eval=True, do_test=True, evaluate_during_training=True, do_lower_case=False, train_batch_size=4, eval_batch_size=32, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=50, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=123456, epoch=5, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', early_stopping_patience=None, min_loss_delta=0.001, dropout_probability=0, n_gpu=1, device=device(type='cuda'), per_gpu_train_batch_size=4, per_gpu_eval_batch_size=32, start_epoch=0, start_step=0)
/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
07/11/2023 21:43:51 - INFO - __main__ -   ***** Running evaluation *****
07/11/2023 21:43:51 - INFO - __main__ -     Num examples = 2732
07/11/2023 21:43:51 - INFO - __main__ -     Batch size = 32
1
07/11/2023 21:45:01 - INFO - __main__ -   ***** Eval results *****
07/11/2023 21:45:01 - INFO - __main__ -     eval_acc = 0.5889
07/11/2023 21:45:01 - INFO - __main__ -     eval_loss = 0.6994
07/11/2023 21:45:12 - INFO - __main__ -   ***** Running Test *****
07/11/2023 21:45:12 - INFO - __main__ -     Num examples = 2732
07/11/2023 21:45:12 - INFO - __main__ -     Batch size = 32
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:06,  1.27it/s]  2%|▏         | 2/86 [00:01<01:05,  1.29it/s]  3%|▎         | 3/86 [00:02<01:03,  1.31it/s]  5%|▍         | 4/86 [00:03<01:02,  1.32it/s]  6%|▌         | 5/86 [00:03<01:01,  1.32it/s]  7%|▋         | 6/86 [00:04<01:00,  1.31it/s]  8%|▊         | 7/86 [00:05<01:00,  1.31it/s]  9%|▉         | 8/86 [00:06<00:59,  1.32it/s] 10%|█         | 9/86 [00:06<00:58,  1.32it/s] 12%|█▏        | 10/86 [00:07<00:57,  1.31it/s] 13%|█▎        | 11/86 [00:08<00:57,  1.30it/s] 14%|█▍        | 12/86 [00:09<00:57,  1.29it/s] 15%|█▌        | 13/86 [00:09<00:56,  1.29it/s] 16%|█▋        | 14/86 [00:10<00:56,  1.29it/s] 17%|█▋        | 15/86 [00:11<00:55,  1.28it/s] 19%|█▊        | 16/86 [00:12<00:54,  1.28it/s] 20%|█▉        | 17/86 [00:13<00:53,  1.28it/s] 21%|██        | 18/86 [00:13<00:53,  1.28it/s] 22%|██▏       | 19/86 [00:14<00:52,  1.27it/s] 23%|██▎       | 20/86 [00:15<00:52,  1.26it/s] 24%|██▍       | 21/86 [00:16<00:51,  1.26it/s] 26%|██▌       | 22/86 [00:17<00:50,  1.26it/s] 27%|██▋       | 23/86 [00:17<00:50,  1.25it/s] 28%|██▊       | 24/86 [00:18<00:49,  1.25it/s] 29%|██▉       | 25/86 [00:19<00:49,  1.24it/s] 30%|███       | 26/86 [00:20<00:48,  1.23it/s] 31%|███▏      | 27/86 [00:21<00:48,  1.23it/s] 33%|███▎      | 28/86 [00:22<00:47,  1.22it/s] 34%|███▎      | 29/86 [00:22<00:47,  1.21it/s] 35%|███▍      | 30/86 [00:23<00:46,  1.21it/s] 36%|███▌      | 31/86 [00:24<00:45,  1.20it/s] 37%|███▋      | 32/86 [00:25<00:44,  1.20it/s] 38%|███▊      | 33/86 [00:26<00:44,  1.19it/s] 40%|███▉      | 34/86 [00:27<00:43,  1.19it/s] 41%|████      | 35/86 [00:27<00:43,  1.18it/s] 42%|████▏     | 36/86 [00:28<00:42,  1.18it/s] 43%|████▎     | 37/86 [00:29<00:41,  1.18it/s] 44%|████▍     | 38/86 [00:30<00:40,  1.18it/s] 45%|████▌     | 39/86 [00:31<00:40,  1.17it/s] 47%|████▋     | 40/86 [00:32<00:39,  1.16it/s] 48%|████▊     | 41/86 [00:33<00:38,  1.17it/s] 49%|████▉     | 42/86 [00:33<00:37,  1.16it/s] 50%|█████     | 43/86 [00:34<00:37,  1.16it/s] 51%|█████     | 44/86 [00:35<00:36,  1.16it/s] 52%|█████▏    | 45/86 [00:36<00:35,  1.15it/s] 53%|█████▎    | 46/86 [00:37<00:34,  1.15it/s] 55%|█████▍    | 47/86 [00:38<00:33,  1.15it/s] 56%|█████▌    | 48/86 [00:39<00:33,  1.15it/s] 57%|█████▋    | 49/86 [00:40<00:32,  1.15it/s] 58%|█████▊    | 50/86 [00:40<00:31,  1.16it/s] 59%|█████▉    | 51/86 [00:41<00:30,  1.15it/s] 60%|██████    | 52/86 [00:42<00:29,  1.16it/s] 62%|██████▏   | 53/86 [00:43<00:28,  1.16it/s] 63%|██████▎   | 54/86 [00:44<00:27,  1.17it/s] 64%|██████▍   | 55/86 [00:45<00:26,  1.17it/s] 65%|██████▌   | 56/86 [00:45<00:25,  1.18it/s] 66%|██████▋   | 57/86 [00:46<00:24,  1.18it/s] 67%|██████▋   | 58/86 [00:47<00:23,  1.18it/s] 69%|██████▊   | 59/86 [00:48<00:22,  1.18it/s] 70%|██████▉   | 60/86 [00:49<00:21,  1.18it/s] 71%|███████   | 61/86 [00:50<00:20,  1.19it/s] 72%|███████▏  | 62/86 [00:51<00:20,  1.19it/s] 73%|███████▎  | 63/86 [00:51<00:19,  1.20it/s] 74%|███████▍  | 64/86 [00:52<00:18,  1.21it/s] 76%|███████▌  | 65/86 [00:53<00:17,  1.22it/s] 77%|███████▋  | 66/86 [00:54<00:16,  1.22it/s] 78%|███████▊  | 67/86 [00:55<00:15,  1.23it/s] 79%|███████▉  | 68/86 [00:55<00:14,  1.23it/s] 80%|████████  | 69/86 [00:56<00:13,  1.23it/s] 81%|████████▏ | 70/86 [00:57<00:12,  1.24it/s] 83%|████████▎ | 71/86 [00:58<00:12,  1.24it/s] 84%|████████▎ | 72/86 [00:59<00:11,  1.24it/s] 85%|████████▍ | 73/86 [00:59<00:10,  1.24it/s] 86%|████████▌ | 74/86 [01:00<00:09,  1.24it/s] 87%|████████▋ | 75/86 [01:01<00:08,  1.25it/s] 88%|████████▊ | 76/86 [01:02<00:08,  1.25it/s] 90%|████████▉ | 77/86 [01:03<00:07,  1.24it/s] 91%|█████████ | 78/86 [01:03<00:06,  1.24it/s] 92%|█████████▏| 79/86 [01:04<00:05,  1.24it/s] 93%|█████████▎| 80/86 [01:05<00:04,  1.26it/s] 94%|█████████▍| 81/86 [01:06<00:03,  1.26it/s] 95%|█████████▌| 82/86 [01:07<00:03,  1.26it/s] 97%|█████████▋| 83/86 [01:07<00:02,  1.27it/s] 98%|█████████▊| 84/86 [01:08<00:01,  1.27it/s] 99%|█████████▉| 85/86 [01:09<00:00,  1.27it/s]100%|██████████| 86/86 [01:09<00:00,  1.53it/s]100%|██████████| 86/86 [01:09<00:00,  1.23it/s]
