# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.
import torch
import torch.nn as nn
import torch
from torch.autograd import Variable
import copy
from torch.nn import CrossEntropyLoss, MSELoss
import torch.nn.functional as F

    
    
class Model(nn.Module):   
    def __init__(self, encoder,config,tokenizer,args):
        super(Model, self).__init__()
        self.encoder = encoder
        self.config=config
        self.tokenizer=tokenizer
        self.args=args
    
        # Define dropout layer, dropout_probability is taken from args.
        self.dropout = nn.Dropout(args.dropout_probability)
    
    @staticmethod
    def kl_divergence(p, q):
        return (p * (p / (q + 1e-10)).log()).sum() #avoiding division by zero

        
    def forward(self, input_ids=None,labels=None,probability_PVS=None): 
        outputs=self.encoder(input_ids,attention_mask=input_ids.ne(1))[0]

        # Apply dropout
        outputs = self.dropout(outputs)

        logits=outputs
        prob=torch.sigmoid(logits)
        
        prob_for_kl=prob.squeeze()
        
        kl_divergences = []
        
        print(probability_PVS)
        if probability_PVS is not None:
            for p_val, q_val in zip(prob_for_kl, probability_PVS):
                p = torch.tensor([p_val, 1-p_val])  
                q = torch.tensor([q_val, 1-q_val])  
                kl_divergence_val = Model.kl_divergence(p, q).item()
                kl_divergences.append(kl_divergence_val)
        
            domain_loss=sum(kl_divergences)/len(kl_divergences)
        else:
            domain_loss=0.0
        #print(domain_loss)
        #print(f"The values of probability_PVS {probability_PVS}, probability from model {prob_for_kl} and KL divergence {kl_divergences}")
        if labels is not None:
            labels=labels.float()
            loss=torch.log(prob[:,0]+1e-10)*labels+torch.log((1-prob)[:,0]+1e-10)*(1-labels)
            loss=-loss.mean()
            
            # Calculating model prediction and our domain guided probability loss using Kullback divergence
            #vulnerability_loss = F.kl_div(prob[:, 0].log(), probability_PVS, reduction='batchmean')
            '''if probability_PVS is not None:
                vulnerability_loss=abs(probability_PVS-prob[:,0])
                vulnerability_loss=vulnerability_loss.mean()
                loss=loss+vulnerability_loss'''
            loss=loss+domain_loss
            return loss,prob
        else:
            return prob
      
        
 
