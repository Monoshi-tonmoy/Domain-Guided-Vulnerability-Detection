{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import json\n",
    "from DomainRulesChecking.DomainRules import DomainRules, PVS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "     ---------------------------------------- 0.0/116.9 kB ? eta -:--:--\n",
      "     ------------------------------------ - 112.6/116.9 kB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 116.9/116.9 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/50/6a/cd59b2e1d6817858e3f332b4128e9246bf8408a7a791f8b77b08633a959d/regex-2023.6.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/3.5 MB 6.5 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.8/3.5 MB 8.6 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 1.3/3.5 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.8/3.5 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 2.3/3.5 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 2.7/3.5 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.2/3.5 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.5/3.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.5/3.5 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp311-cp311-win_amd64.whl (263 kB)\n",
      "     ---------------------------------------- 0.0/263.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 263.7/263.7 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "   ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.4 MB 16.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.9/7.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.2/7.4 MB 9.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.8/7.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.4/7.4 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.9/7.4 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.4/7.4 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.9/7.4 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.4/7.4 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.0/7.4 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.5/7.4 MB 11.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.1/7.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.6/7.4 MB 11.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.2/7.4 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.4/7.4 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.4/7.4 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.4/7.4 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.8/268.8 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 268.0/268.0 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB 4.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, safetensors, regex, fsspec, huggingface-hub, transformers\n",
      "Successfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/02/bd/673947dde6b3a43f4ffc3abaf103947c4fb574ac8b7c32747f2421f1f7c9/tensorboardX-2.6.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.1-py2.py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboardX) (1.25.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboardX) (23.1)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in c:\\users\\tonmo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboardX) (4.23.4)\n",
      "Downloading tensorboardX-2.6.1-py2.py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.6/101.6 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from model import Model\n",
    "cpu_cont = multiprocessing.cpu_count()\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          BertConfig, BertForMaskedLM, BertTokenizer, BertForSequenceClassification,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                          OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          DistilBertConfig, DistilBertForMaskedLM, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_tokens,\n",
    "                 input_ids,\n",
    "                 idx,\n",
    "                 label,\n",
    "                 probability_PVS,\n",
    "\n",
    "    ):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.idx=str(idx)\n",
    "        self.label=label\n",
    "        self.probability_PVS=probability_PVS\n",
    "\n",
    "        \n",
    "def convert_examples_to_features(js,tokenizer,args):\n",
    "    #source\n",
    "    code=' '.join(js['func'].split())\n",
    "    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n",
    "    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "    padding_length = args.block_size - len(source_ids)\n",
    "    source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "\n",
    "    #Using Knowledge Rules (PVS)\n",
    "    pvs=PVS(code)  #Creating instance of our domainrule class\n",
    "    probability_PVS=pvs.get_pvs_v2()  #returning whether samples contain Potential Vulnerable Statement(PVS) or not\n",
    "    \n",
    "    \n",
    "    if probability_PVS:\n",
    "        probability_PVS=1\n",
    "    else:\n",
    "        probability_PVS=0\n",
    "    \n",
    "    return InputFeatures(source_tokens,source_ids,js['idx'],js['target'],probability_PVS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, file_path=None):\n",
    "        self.examples = []\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                js=json.loads(line.strip())\n",
    "                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n",
    "        if 'train' in file_path:\n",
    "            for idx, example in enumerate(self.examples[:3]):\n",
    "                    logger.info(\"*** Example ***\")\n",
    "                    logger.info(\"idx: {}\".format(idx))\n",
    "                    logger.info(\"label: {}\".format(example.label))\n",
    "                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n",
    "                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):       \n",
    "        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label), torch.tensor(self.examples[i].probability_PVS)\n",
    "            \n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\" \n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \n",
    "                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n",
    "    args.max_steps=args.epoch*len( train_dataloader)\n",
    "    args.save_steps=len( train_dataloader)\n",
    "    args.warmup_steps=len( train_dataloader)\n",
    "    args.logging_steps=len( train_dataloader)\n",
    "    args.num_train_epochs=args.epoch\n",
    "    model.to(args.device)\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
    "                                                num_training_steps=args.max_steps)\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                          output_device=args.local_rank,\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    scheduler_last = os.path.join(checkpoint_last, 'scheduler.pt')\n",
    "    optimizer_last = os.path.join(checkpoint_last, 'optimizer.pt')\n",
    "    if os.path.exists(scheduler_last):\n",
    "        scheduler.load_state_dict(torch.load(scheduler_last))\n",
    "    if os.path.exists(optimizer_last):\n",
    "        optimizer.load_state_dict(torch.load(optimizer_last))\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
    "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", args.max_steps)\n",
    "    \n",
    "    global_step = args.start_step\n",
    "    tr_loss, logging_loss,avg_loss,tr_nb,tr_num,train_loss = 0.0, 0.0,0.0,0,0,0\n",
    "    best_mrr=0.0\n",
    "    best_acc=0.0\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Initialize early stopping parameters at the start of training\n",
    "    early_stopping_counter = 0\n",
    "    best_loss = None\n",
    " \n",
    "    for idx in range(args.start_epoch, int(args.num_train_epochs)): \n",
    "        bar = tqdm(train_dataloader,total=len(train_dataloader))\n",
    "        tr_num=0\n",
    "        train_loss=0\n",
    "        for step, batch in enumerate(bar):\n",
    "            inputs = batch[0].to(args.device)  \n",
    "            #print(\"Inputs are Here:\\n\")\n",
    "            #print(inputs)\n",
    "            labels=batch[1].to(args.device)\n",
    "            #print(\"Labels are here:\\n\")\n",
    "            #print(labels)\n",
    "            probability_PVS=batch[2].to(args.device) \n",
    "            #print(\"Probability PVS\\n\")\n",
    "            #print(probability_PVS)\n",
    "            model.train()\n",
    "            loss,logits = model(inputs,labels,probability_PVS)\n",
    "\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_num+=1\n",
    "            train_loss+=loss.item()\n",
    "            if avg_loss==0:\n",
    "                avg_loss=tr_loss\n",
    "            avg_loss=round(train_loss/tr_num,5)\n",
    "            bar.set_description(\"epoch {} loss {}\".format(idx,avg_loss))\n",
    "\n",
    "                \n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()  \n",
    "                global_step += 1\n",
    "                output_flag=True\n",
    "                avg_loss=round(np.exp((tr_loss - logging_loss) /(global_step- tr_nb)),4)\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    logging_loss = tr_loss\n",
    "                    tr_nb=global_step\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    \n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer,eval_when_training=True)\n",
    "                        for key, value in results.items():\n",
    "                            logger.info(\"  %s = %s\", key, round(value,4))                    \n",
    "                        # Save model checkpoint\n",
    "                        \n",
    "                    if results['eval_acc']>best_acc:\n",
    "                        best_acc=results['eval_acc']\n",
    "                        logger.info(\"  \"+\"*\"*20)  \n",
    "                        logger.info(\"  Best acc:%s\",round(best_acc,4))\n",
    "                        logger.info(\"  \"+\"*\"*20)                          \n",
    "                        \n",
    "                        checkpoint_prefix = 'checkpoint-best-acc'\n",
    "                        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))                        \n",
    "                        if not os.path.exists(output_dir):\n",
    "                            os.makedirs(output_dir)                        \n",
    "                        model_to_save = model.module if hasattr(model,'module') else model\n",
    "                        output_dir = os.path.join(output_dir, '{}'.format('model.bin')) \n",
    "                        torch.save(model_to_save.state_dict(), output_dir)\n",
    "                        logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = train_loss / tr_num\n",
    "\n",
    "        # Check for early stopping condition\n",
    "        if args.early_stopping_patience is not None:\n",
    "            if best_loss is None or avg_loss < best_loss - args.min_loss_delta:\n",
    "                best_loss = avg_loss\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= args.early_stopping_patience:\n",
    "                    logger.info(\"Early stopping\")\n",
    "                    break  # Exit the loop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --train_data_file TRAIN_DATA_FILE\n",
      "                             --output_dir OUTPUT_DIR\n",
      "                             [--eval_data_file EVAL_DATA_FILE]\n",
      "                             [--test_data_file TEST_DATA_FILE]\n",
      "                             [--model_type MODEL_TYPE]\n",
      "                             [--model_name_or_path MODEL_NAME_OR_PATH] [--mlm]\n",
      "                             [--mlm_probability MLM_PROBABILITY]\n",
      "                             [--config_name CONFIG_NAME]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             [--cache_dir CACHE_DIR] [--block_size BLOCK_SIZE]\n",
      "                             [--do_train] [--do_eval] [--do_test]\n",
      "                             [--evaluate_during_training] [--do_lower_case]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--eval_all_checkpoints] [--no_cuda]\n",
      "                             [--overwrite_output_dir] [--overwrite_cache]\n",
      "                             [--seed SEED] [--epoch EPOCH] [--fp16]\n",
      "                             [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
      "                             [--server_port SERVER_PORT]\n",
      "                             [--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
      "                             [--min_loss_delta MIN_LOSS_DELTA]\n",
      "                             [--dropout_probability DROPOUT_PROBABILITY]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\tonmo\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-436WMi3KnJXhqlv.json could match --fp16, --fp16_opt_level\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args([])\n",
    "    args.output_dir=./saved_models\n",
    "    args.output_dir = \"path/to/output/dir\"\n",
    "    args.model_type = \"gpt2\"\n",
    "    args.model_name_or_path = \"gpt2\"\n",
    "    args.do_train = True\n",
    "    args.num_train_epochs = 3  # Number of training epochs you want to run\n",
    "    args.block_size = 256  # Set the appropriate block size\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = 1 if args.no_cuda else torch.cuda.device_count()\n",
    "        print(args.n_gpu)\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "    args.per_gpu_train_batch_size = args.train_batch_size // args.n_gpu\n",
    "    args.per_gpu_eval_batch_size = args.eval_batch_size // args.n_gpu\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "    args.start_epoch = 0\n",
    "    args.start_step = 0\n",
    "    checkpoint_last = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "    if os.path.exists(checkpoint_last) and os.listdir(checkpoint_last):\n",
    "        args.model_name_or_path = os.path.join(checkpoint_last, 'pytorch_model.bin')\n",
    "        args.config_name = os.path.join(checkpoint_last, 'config.json')\n",
    "        idx_file = os.path.join(checkpoint_last, 'idx_file.txt')\n",
    "        with open(idx_file, encoding='utf-8') as idxf:\n",
    "            args.start_epoch = int(idxf.readlines()[0].strip()) + 1\n",
    "\n",
    "        step_file = os.path.join(checkpoint_last, 'step_file.txt')\n",
    "        if os.path.exists(step_file):\n",
    "            with open(step_file, encoding='utf-8') as stepf:\n",
    "                args.start_step = int(stepf.readlines()[0].strip())\n",
    "\n",
    "        logger.info(\"reload model from {}, resume from {} epoch\".format(checkpoint_last, args.start_epoch))\n",
    "\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                          cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "    config.num_labels=1\n",
    "    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name,\n",
    "                                                do_lower_case=args.do_lower_case,\n",
    "                                                cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "    if args.block_size <= 0:\n",
    "        args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "    args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
    "    if args.model_name_or_path:\n",
    "        model = model_class.from_pretrained(args.model_name_or_path,\n",
    "                                            from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "                                            config=config,\n",
    "                                            cache_dir=args.cache_dir if args.cache_dir else None)    \n",
    "    else:\n",
    "        model = model_class(config)\n",
    "\n",
    "    model=Model(model,config,tokenizer,args)\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        if args.local_rank not in [-1, 0]:\n",
    "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n",
    "        if args.local_rank == 0:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        train(args, train_dataset, model, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
    "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n",
    "            model.load_state_dict(torch.load(output_dir))      \n",
    "            model.to(args.device)\n",
    "            result=evaluate(args, model, tokenizer)\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(round(result[key],4)))\n",
    "            \n",
    "    if args.do_test and args.local_rank in [-1, 0]:\n",
    "            checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n",
    "            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n",
    "            model.load_state_dict(torch.load(output_dir))                  \n",
    "            model.to(args.device)\n",
    "            test(args, model, tokenizer)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "probabilities_list = [0.4569, 0.4578, 0.4291, 0.4388, 0.4617, 0.4937, 0.4657, 0.5202, 0.4876, 0.4833, 0.4251, 0.4892, 0.4997, 0.5118, 0.4894, 0.4745, 0.4833, 0.4986, 0.5135, 0.5339, 0.5068, 0.4804, 0.5072, 0.5686, 0.5263, 0.4516, 0.4833, 0.4823, 0.4970, 0.4842, 0.5233, 0.4413]\n",
    "domain_rules_list = [0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
    "\n",
    "# Convert the lists to PyTorch tensors\n",
    "probabilities_tensor = torch.tensor(probabilities_list).view(-1, 1)\n",
    "domain_rules_tensor = torch.tensor(domain_rules_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4569],\n",
      "        [0.4578],\n",
      "        [0.4291],\n",
      "        [0.4388],\n",
      "        [0.4617],\n",
      "        [0.4937],\n",
      "        [0.4657],\n",
      "        [0.5202],\n",
      "        [0.4876],\n",
      "        [0.4833],\n",
      "        [0.4251],\n",
      "        [0.4892],\n",
      "        [0.4997],\n",
      "        [0.5118],\n",
      "        [0.4894],\n",
      "        [0.4745],\n",
      "        [0.4833],\n",
      "        [0.4986],\n",
      "        [0.5135],\n",
      "        [0.5339],\n",
      "        [0.5068],\n",
      "        [0.4804],\n",
      "        [0.5072],\n",
      "        [0.5686],\n",
      "        [0.5263],\n",
      "        [0.4516],\n",
      "        [0.4833],\n",
      "        [0.4823],\n",
      "        [0.4970],\n",
      "        [0.4842],\n",
      "        [0.5233],\n",
      "        [0.4413]])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(domain_rules_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5371105074882507\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming you have the following tensors\n",
    "# probabilities_tensor: Tensor containing the probability distribution of shape (batch_size, 1)\n",
    "# domain_rules_tensor: Tensor containing the probability from domain rules of shape (batch_size,)\n",
    "\n",
    "# Convert the domain_rules_tensor to a probability tensor\n",
    "domain_rules_tensor = domain_rules_tensor.float()\n",
    "\n",
    "# Calculate the KL divergence for each sample\n",
    "kl_divergence = F.kl_div(probabilities_tensor.log(), domain_rules_tensor.view(-1, 1), reduction='none')\n",
    "\n",
    "# Take the average of the KL divergences across the batch\n",
    "average_kl_divergence = kl_divergence.mean()\n",
    "\n",
    "print(average_kl_divergence.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [0.7813],\n",
      "        [0.8461],\n",
      "        [0.8237],\n",
      "        [0.7728],\n",
      "        [0.7058],\n",
      "        [0.7642],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.7271],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.6937],\n",
      "        [0.6698],\n",
      "        [0.7146],\n",
      "        [0.7455],\n",
      "        [0.7271],\n",
      "        [0.6960],\n",
      "        [0.0000],\n",
      "        [0.6275],\n",
      "        [0.6796],\n",
      "        [0.7331],\n",
      "        [0.6788],\n",
      "        [0.5646],\n",
      "        [0.6419],\n",
      "        [0.7950],\n",
      "        [0.7271],\n",
      "        [0.0000],\n",
      "        [0.6992],\n",
      "        [0.7253],\n",
      "        [0.6476],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5431, 0.5422, 0.5709, 0.5612, 0.5383, 0.5063, 0.5343, 0.4798, 0.5124,\n",
      "        0.5167, 0.5749, 0.5108, 0.5003, 0.4882, 0.5106, 0.5255, 0.5167, 0.5014,\n",
      "        0.4865, 0.4661, 0.4932, 0.5196, 0.4928, 0.4314, 0.4737, 0.5484, 0.5167,\n",
      "        0.5177, 0.5030, 0.5158, 0.4767, 0.5587])\n"
     ]
    }
   ],
   "source": [
    "print((1-probabilities_tensor)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "Sample 1: KL Divergence = 0.3779284358024597\n",
      "Sample 2: KL Divergence = 0.11053511500358582\n",
      "Sample 3: KL Divergence = 0.38592609763145447\n",
      "Sample 4: KL Divergence = 0.40639352798461914\n",
      "Sample 5: KL Divergence = 0.012796789407730103\n",
      "Sample 6: KL Divergence = 0.023133456707000732\n",
      "Sample 7: KL Divergence = 0.13454179465770721\n",
      "Sample 8: KL Divergence = 0.1773088127374649\n",
      "Sample 9: KL Divergence = 2.5756349563598633\n",
      "Sample 10: KL Divergence = 0.004014786332845688\n",
      "Sample 11: KL Divergence = 0.22702784836292267\n",
      "Sample 12: KL Divergence = 0.11040644347667694\n",
      "Sample 13: KL Divergence = 0.0039765238761901855\n",
      "Sample 14: KL Divergence = 0.1706172078847885\n",
      "Sample 15: KL Divergence = 1.49001944065094\n",
      "Sample 16: KL Divergence = 0.22047950327396393\n",
      "Sample 17: KL Divergence = 0.00575796514749527\n",
      "Sample 18: KL Divergence = 0.379465252161026\n",
      "Sample 19: KL Divergence = 0.03260117769241333\n",
      "Sample 20: KL Divergence = 0.24095389246940613\n",
      "Sample 21: KL Divergence = 0.0013139043003320694\n",
      "Sample 22: KL Divergence = 1.5179672241210938\n",
      "Sample 23: KL Divergence = 0.9950628280639648\n",
      "Sample 24: KL Divergence = 0.11391739547252655\n",
      "Sample 25: KL Divergence = 0.025306381285190582\n",
      "Sample 26: KL Divergence = 0.17948374152183533\n",
      "Sample 27: KL Divergence = 0.00014480389654636383\n",
      "Sample 28: KL Divergence = 0.2040439248085022\n",
      "Sample 29: KL Divergence = 0.0018753726035356522\n",
      "Sample 30: KL Divergence = 0.24416811764240265\n",
      "Sample 31: KL Divergence = 2.3156182765960693\n",
      "Sample 32: KL Divergence = 0.2805977761745453\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return (p * (p / (q + 1e-10)).log()).sum()\n",
    "\n",
    "# Given probability distribution tensor of shape (32, 1)\n",
    "probabilities_list = torch.tensor([[0.4], [0.4578], [0.4291], [0.4388], [0.4617], [0.4937], [0.4657], [0.5202], [0.4876], [0.4833], [0.4251], [0.4892], [0.4997], [0.5118], [0.4894], [0.4745], [0.4833], [0.4986], [0.5135], [0.5339], [0.5068], [0.4804], [0.5072], [0.5686], [0.5263], [0.4516], [0.4833], [0.4823], [0.4970], [0.4842], [0.5233], [0.4413]])\n",
    "\n",
    "# Given domain rules tensor of shape (32,)\n",
    "domain_rules_list = torch.tensor([0.7984, 0.2417, 0.0945, 0.8356, 0.3831, 0.6002, 0.7137, 0.7889, 0.9983, 0.4387, 0.1459, 0.7132, 0.4552, 0.2406, 0.0117, 0.7793, 0.5369, 0.8638, 0.6385, 0.2159, 0.5324, 0.9856, 0.9663, 0.3352, 0.4145, 0.1886, 0.4748, 0.1973, 0.5276, 0.7993, 0.0032, 0.7856])\n",
    "\n",
    "# Calculate the probability of \"no\" for each sample\n",
    "p_no_tensor = 1 - probabilities_list\n",
    "\n",
    "print(p_no_tensor.size())\n",
    "\n",
    "# Convert the probability tensors to shape (32,)\n",
    "p_yes_tensor = probabilities_list.squeeze()\n",
    "p_no_tensor = p_no_tensor.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate KL divergence for each sample\n",
    "kl_divergences = []\n",
    "for p_yes_val, p_no_val, q_val in zip(p_yes_tensor, p_no_tensor, domain_rules_list):\n",
    "    p = torch.tensor([p_yes_val, p_no_val])  # Add epsilon to prevent division by zero\n",
    "    q = torch.tensor([q_val, 1-q_val])  # Add epsilon to prevent division by zero\n",
    "    kl_divergence_val = kl_divergence(p, q).item()\n",
    "    kl_divergences.append(kl_divergence_val)\n",
    "\n",
    "# Print the KL divergence for each sample\n",
    "for i, kl_divergence_val in enumerate(kl_divergences):\n",
    "    print(f\"Sample {i + 1}: KL Divergence = {kl_divergence_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(q[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[0.4,0.6]\n",
    "q=[0.7,0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2770580311769584\n"
     ]
    }
   ],
   "source": [
    "p=[0.4,0.6]\n",
    "q=[0.7,0.3]\n",
    "\n",
    "import math\n",
    "sum=0.0\n",
    "\n",
    "for i in range(len(p)):\n",
    "    sum=sum+(p[i]*math.log2(p[i]/q[i]))\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence = 0.1920420378446579\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return (p * (p / (q + 1e-10)).log()).sum()\n",
    "\n",
    "# Given probability distribution tensor of shape (2,)\n",
    "p = torch.tensor([0.4, 0.6])\n",
    "\n",
    "# Given domain rules tensor of shape (2,)\n",
    "q = torch.tensor([0.7, 0.3])\n",
    "\n",
    "# Calculate the KL divergence\n",
    "kl_divergence_val = kl_divergence(p, q).item()\n",
    "\n",
    "print(\"KL Divergence =\", kl_divergence_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence = 0.27705806493759155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return (p * (p / (q + 1e-10)).log() / torch.log(torch.tensor(2.0))).sum()\n",
    "\n",
    "# Given probability distribution tensor of shape (2,)\n",
    "p = torch.tensor([0.4, 0.6])\n",
    "\n",
    "# Given domain rules tensor of shape (2,)\n",
    "q = torch.tensor([0.7, 0.3])\n",
    "\n",
    "# Calculate the KL divergence using log2\n",
    "kl_divergence_val = kl_divergence(p, q).item()\n",
    "\n",
    "print(\"KL Divergence =\", kl_divergence_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python run.py `\n",
    ">>     --output_dir=./saved_models `\n",
    ">>     --model_type=roberta `\n",
    ">>     --tokenizer_name=microsoft/codebert-base `\n",
    ">>     --model_name_or_path=microsoft/codebert-base `\n",
    ">>     --do_train `\n",
    ">>     --train_data_file=../dataset/train.jsonl `\n",
    ">>     --eval_data_file=../dataset/valid.jsonl `\n",
    ">>     --test_data_file=../dataset/test.jsonl `\n",
    ">>     --epoch 1 `\n",
    ">>     --block_size 400 `\n",
    ">>     --train_batch_size 32 `\n",
    ">>     --eval_batch_size 64 `\n",
    ">>     --learning_rate 2e-5 `\n",
    ">>     --max_grad_norm 1.0 `\n",
    ">>     --evaluate_during_training `\n",
    ">>     --no_cuda `\n",
    ">>     --seed 123456 2>&1 | tee train.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
