{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 41\n",
      "Marked tokens: 1\n",
      "Unmarked tokens: 40\n"
     ]
    }
   ],
   "source": [
    "from DomainRules import DomainRules, PVS\n",
    "\n",
    "def count_tokens(code_snippet):\n",
    "    # Create an instance of the PVS class\n",
    "    pvs = PVS(code_snippet)\n",
    "\n",
    "    # Get the list of boolean values from the walk function\n",
    "    marked_tokens = pvs.get_pvs_v2()\n",
    "\n",
    "    # Count the total number of tokens\n",
    "    total_tokens = len(marked_tokens)\n",
    "\n",
    "    # Count the number of marked tokens\n",
    "    marked_count = sum(marked_tokens)\n",
    "\n",
    "    # Count the number of unmarked tokens\n",
    "    unmarked_count = total_tokens - marked_count\n",
    "\n",
    "    return total_tokens, marked_count, unmarked_count\n",
    "\n",
    "# Example usage\n",
    "sample=\"\"\"int main()\n",
    "        {\n",
    "            int a;\n",
    "            printf(\"hello\")\n",
    "            scanf(\"%d\",a);\n",
    "            printf(a);\n",
    "        }\n",
    "    \"\"\"\n",
    "total_tokens, marked_count, unmarked_count = count_tokens(sample)\n",
    "\n",
    "print(\"Total tokens:\", total_tokens)\n",
    "print(\"Marked tokens:\", marked_count)\n",
    "print(\"Unmarked tokens:\", unmarked_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Node type=translation_unit, start_point=(0, 0), end_point=(7, 4)>\n"
     ]
    }
   ],
   "source": [
    "sample=\"\"\"int main()\n",
    "        {\n",
    "            int a;\n",
    "            printf(\"hello\")\n",
    "            scanf(\"%d\",a);\n",
    "            printf(a);\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "pvs=PVS(sample)\n",
    "tree=pvs.making_tree()\n",
    "\n",
    "print(tree.root_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter\n",
    "\n",
    "def counting_tokens(code):\n",
    "    pvs = PVS(code)\n",
    "    tree = pvs.making_tree()\n",
    "\n",
    "    total_tokens = 0\n",
    "    marked_count = 0\n",
    "\n",
    "    def traverse(node):\n",
    "        nonlocal total_tokens, marked_count\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        # Increment the total_tokens count for each token (including whitespaces)\n",
    "        total_tokens += 1\n",
    "\n",
    "        # Check if the token should be marked\n",
    "        if pvs.mark_tokens(node):\n",
    "            # Mark the token (modify this condition based on your marking criteria)\n",
    "            marked_count += 1\n",
    "\n",
    "        # Recursively traverse the child nodes\n",
    "        for child in node.children:\n",
    "            traverse(child)\n",
    "\n",
    "    # Start traversal from the root node\n",
    "    traverse(tree.root_node)\n",
    "\n",
    "    return total_tokens, marked_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 45\n",
      "Marked tokens: 1\n",
      "Unmarked tokens: 44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample code\n",
    "sample = \"\"\"int main()\n",
    "        {\n",
    "            int a;\n",
    "            printf(\"hello\")\n",
    "            scanf(\"%d\",a);\n",
    "            printf(a);\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "# Count tokens and marked tokens\n",
    "total_tokens, marked_count = counting_tokens(sample)\n",
    "\n",
    "print(\"Total tokens:\", total_tokens)\n",
    "print(\"Marked tokens:\", marked_count)\n",
    "print(\"Unmarked tokens:\", total_tokens - marked_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous implementation\n",
    "\n",
    "\n",
    "from tree_sitter_languages import get_parser\n",
    "import tree_sitter\n",
    "\n",
    "\n",
    "\n",
    "class DomainRules:\n",
    "    def __init__(self, training_sample):\n",
    "        self.training_sample = training_sample\n",
    "        self.parser = get_parser(\"c\")\n",
    "        self.tree = None\n",
    "    \n",
    "    def making_tree(self):\n",
    "        if isinstance(self.training_sample, str):\n",
    "            self.training_sample = self.training_sample.encode()\n",
    "        self.tree = self.parser.parse(self.training_sample)\n",
    "        return self.tree\n",
    "\n",
    "class PVS(DomainRules):\n",
    "    def __init__(self, training_sample):\n",
    "        super().__init__(training_sample)\n",
    "        self.training_sample = training_sample\n",
    "        \n",
    "    def is_type(self, typename):\n",
    "        def fn(n):\n",
    "            return n.type == typename\n",
    "        return fn\n",
    "\n",
    "    def get_children(self, node, fn, reverse=False):\n",
    "        it = node.children\n",
    "        if reverse:\n",
    "            it = reversed(it)\n",
    "        if isinstance(fn, str):\n",
    "            fn_str = str(fn)\n",
    "            fn = lambda n: n.type == fn_str\n",
    "        return [c for c in it if fn(c)]\n",
    "\n",
    "    def get_child(self, node, fn, reverse=False):\n",
    "        return next(iter(self.get_children(node, fn, reverse=reverse)), None)\n",
    "\n",
    "    def get_child_at_index(self, node, i):\n",
    "        c = node.children[i]\n",
    "        while c.type == \"comment\":\n",
    "            i += 1\n",
    "            c = node.children[i]\n",
    "        return c\n",
    "\n",
    "    def serialize_node(self, node):\n",
    "        return {\n",
    "            \"text\": node.text.decode(),\n",
    "            \"start_point\": node.start_point,\n",
    "            \"end_point\": node.end_point,\n",
    "            \"id\": node.id,\n",
    "        }\n",
    "\n",
    "\n",
    "    def mark_tokens(self, n, abstract=False):\n",
    "        if n.type == \"call_expression\":\n",
    "            ident = self.get_child(n, \"identifier\")\n",
    "            args = self.get_child(n, \"argument_list\")\n",
    "            assert args is not None, (\"call\", n, n.children)\n",
    "            if ident is None:\n",
    "                pass\n",
    "            else:\n",
    "                ident_name = ident.text.decode()\n",
    "                alloc = [\n",
    "                    \"malloc\",\n",
    "                    \"calloc\",\n",
    "                    \"realloc\",\n",
    "                    \"kalloc\",\n",
    "                    \"kcalloc\",\n",
    "                    \"krealloc\",\n",
    "                    \"valloc\",\n",
    "                    \"vcalloc\",\n",
    "                    \"vrealloc\",\n",
    "                ]\n",
    "\n",
    "                free = [\n",
    "                    \"free\",\n",
    "                    \"kfree\",\n",
    "                    \"free_sized\",\n",
    "                    \"free_aligned_sized\",\n",
    "                ]\n",
    "\n",
    "                bof = [\n",
    "                    \"gets\",\n",
    "                    \"puts\",\n",
    "                    \"scanf\",\n",
    "                    \"sprintf\",\n",
    "                    \"strlen\",\n",
    "                    \"strcpy\",\n",
    "                    \"strncpy\",\n",
    "                    \"strcat\",\n",
    "                    \"strncat\",\n",
    "                ]\n",
    "                if ident_name in alloc:\n",
    "                    return True\n",
    "                elif ident_name in free:\n",
    "                    return True\n",
    "                elif ident_name in bof:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        # assignment expression\n",
    "        elif n.type in (\"init_declarator\", \"assignment_expression\"):\n",
    "            # get and mark '=' sign\n",
    "            operator = self.get_child_at_index(n, 1)\n",
    "            assert not operator.is_named, (\"assignment\", n, n.children)\n",
    "            if operator.type in (\"+=\", \"-=\"):\n",
    "                return True\n",
    "            if operator.type in (\"/=\", \"%=\"):\n",
    "                return True\n",
    "        # binary operator expression\n",
    "        elif n.type in (\"update_expression\",):\n",
    "            operator = self.get_child(n, lambda c: c.type in (\"++\", \"--\"))\n",
    "            assert not operator.is_named, (\"update\", n, n.children)\n",
    "            return True\n",
    "        elif n.type in (\"binary_expression\",):\n",
    "            operator = self.get_child_at_index(n, 1)\n",
    "            assert not operator.is_named, (\"binary_operator\", n, n.children)\n",
    "            if operator.type in (\"+\", \"-\", \"*\"):\n",
    "                return True\n",
    "            if operator.type in (\"/\", \"%\"):\n",
    "                return True\n",
    "        # array/pointer operator expression (*, ->, [])\n",
    "        elif n.type in (\"pointer_expression\"):\n",
    "            operator = self.get_child_at_index(n, 0)\n",
    "            if operator.type in (\"pointer_expression\", \"&\"):\n",
    "                return False\n",
    "            assert operator.type == \"*\", (\"*\", n, n.children)\n",
    "            return True\n",
    "        \n",
    "        elif n.type in (\"field_expression\"):\n",
    "            operator = self.get_child_at_index(n, 1)\n",
    "            if operator.type in (\".\",):\n",
    "                return False\n",
    "            assert operator.type == \"->\", (\"->\", n, n.children)\n",
    "            return True\n",
    "        \n",
    "        elif n.type in (\"subscript_expression\"):\n",
    "            operator1 = self.get_child_at_index(n, 1)\n",
    "            operator2 = self.get_child_at_index(n, 3)\n",
    "            assert operator1.type == \"[\" and operator2.type == \"]\", (\"index\", n, n.children)\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def walk(self, tree, abstract=False, enter_compound=True):\n",
    "        if isinstance(tree, tree_sitter.Tree):\n",
    "            root = tree.root_node\n",
    "        else:\n",
    "            root = tree\n",
    "        q = [(root, 0)]\n",
    "        \n",
    "        mark=[]\n",
    "        total_tokens=0\n",
    "        while len(q) > 0:\n",
    "            n, indent = q.pop()\n",
    "            print(f\"printing node {n}\")\n",
    "            if n.type==\"whitespace\":\n",
    "                continue\n",
    "            total_tokens+=1\n",
    "            try:\n",
    "                mt=self.mark_tokens(n, abstract=abstract)\n",
    "            except AssertionError as e:\n",
    "                if not any(c.has_error for c in n.children):\n",
    "                    print(\"***AssertionError***\", e)\n",
    "                mt = None\n",
    "            if mt is not None:\n",
    "                mark.append(mt)\n",
    "            if n.type not in (\"string_literal\", \"char_literal\"):\n",
    "                if n.type == \"compound_statement\" and not enter_compound:\n",
    "                    continue\n",
    "                next_indent = [indent + 1] * len(n.children)\n",
    "                q.extend(reversed(list(zip(n.children, next_indent))))\n",
    "        return mark, total_tokens\n",
    "\n",
    "    def get_pvs_v2(self, abstract=False):\n",
    "        self.tree = self.making_tree()\n",
    "        self.pvs,true_count = self.walk(self.tree, abstract=abstract, enter_compound=True)\n",
    "        #true_count=self.pvs.count(True)\n",
    "        total_tokens=len(self.pvs)\n",
    "        print(f\"Total PVS tokens {true_count} and total number of tokens {total_tokens}\")\n",
    "        #pvs_detected = any(self.pvs)  # Check if any mark is True in the list\n",
    "        pvs_probability=true_count/total_tokens\n",
    "        return pvs_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "project_dir = f\"{os.getcwd().split('model-analysis')[0]}model-analysis\"\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "import argparse\n",
    "from tree_sitter_languages import get_parser\n",
    "import tree_sitter\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"dsname\")\n",
    "# parser.add_argument(\"--abstract\", action=\"store_true\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# dsname = args.dsname\n",
    "# abstract = args.abstract\n",
    "# # ds = get_df(dsname)\n",
    "# abstract=None\n",
    "# # %%\n",
    "\n",
    "def is_type(typename):\n",
    "    def fn(n):\n",
    "        return n.type == typename\n",
    "    return fn\n",
    "\n",
    "def get_children(node, fn, reverse=False):\n",
    "    it = node.children\n",
    "    if reverse:\n",
    "        it = reversed(it)\n",
    "    if isinstance(fn, str):\n",
    "        fn_str = str(fn)\n",
    "        fn = lambda n: n.type == fn_str\n",
    "    return [c for c in it if fn(c)]\n",
    "\n",
    "def get_child(node, fn, reverse=False):\n",
    "    return next(iter(get_children(node, fn, reverse=reverse)), None)\n",
    "\n",
    "def get_child_at_index(node, i):\n",
    "    c = node.children[i]\n",
    "    while c.type == \"comment\":\n",
    "        i += 1\n",
    "        c = node.children[i]\n",
    "    return c\n",
    "\n",
    "def serialize_node(node):\n",
    "    return {\n",
    "        \"text\": node.text.decode(),\n",
    "        \"start_point\": node.start_point,\n",
    "        \"end_point\": node.end_point,\n",
    "        \"id\": node.id,\n",
    "    }\n",
    "\n",
    "def mark_tokens(n, abstract=False):\n",
    "    # call expression\n",
    "    if n.type == \"call_expression\":\n",
    "        ident = get_child(n, \"identifier\")\n",
    "        args = get_child(n, \"argument_list\")\n",
    "        assert args is not None, (\"call\", n, n.children)\n",
    "        # left_parens = get_child(args, \"(\")\n",
    "        # assert left_parens is not None and left_parens.type == \"(\", (\"call\", n, n.children)\n",
    "        # right_parens = get_child(args, \")\")\n",
    "        # assert right_parens is not None and right_parens.type == \")\", (\"call\", n, n.children)\n",
    "        if ident is None:\n",
    "            # print(\"No identifier:\", n.text.decode())\n",
    "            pass\n",
    "        else:\n",
    "            ident_name = ident.text.decode()\n",
    "            alloc = [\n",
    "                \"malloc\",\n",
    "                \"calloc\",\n",
    "                \"realloc\",\n",
    "                \"kalloc\",\n",
    "                \"kcalloc\",\n",
    "                \"krealloc\",\n",
    "                \"valloc\",\n",
    "                \"vcalloc\",\n",
    "                \"vrealloc\",\n",
    "            ]\n",
    "\n",
    "            free = [\n",
    "                \"free\",\n",
    "                \"kfree\",\n",
    "                \"free_sized\",\n",
    "                \"free_aligned_sized\",\n",
    "            ]\n",
    "\n",
    "            bof = [\n",
    "                \"gets\",\n",
    "                \"puts\",\n",
    "                \"scanf\",\n",
    "                \"sprintf\",\n",
    "                \"strlen\",\n",
    "                \"strcpy\",\n",
    "                \"strncpy\",\n",
    "                \"strcat\",\n",
    "                \"strncat\",\n",
    "            ]\n",
    "            if ident_name in alloc:\n",
    "                kind = \"alloc\"\n",
    "            elif ident_name in free:\n",
    "                kind = \"free\"\n",
    "            elif ident_name in bof:\n",
    "                kind = \"overflow\"\n",
    "            else:\n",
    "                return\n",
    "            return {\n",
    "                \"kind\": kind,\n",
    "                \"parent\": serialize_node(n),\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"kind\": \"identifier\",\n",
    "                        \"token\": serialize_node(ident),\n",
    "                    },\n",
    "                    # {\n",
    "                    #     \"kind\": \"lparens\",\n",
    "                    #     \"token\": serialize_node(left_parens),\n",
    "                    # },\n",
    "                    # {\n",
    "                    #     \"kind\": \"rparens\",\n",
    "                    #     \"token\": serialize_node(right_parens),\n",
    "                    # },\n",
    "                ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "            }\n",
    "    # assignment expression\n",
    "    elif n.type in (\"init_declarator\", \"assignment_expression\"):\n",
    "        # get and mark '=' sign\n",
    "        operator = get_child_at_index(n, 1)\n",
    "        assert not operator.is_named, (\"assignment\", n, n.children)\n",
    "        if operator.type in (\"+=\", \"-=\",):\n",
    "            return {\n",
    "                \"kind\": \"overflow/underflow\",\n",
    "                \"parent\": serialize_node(n),\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"kind\": \"operator\",\n",
    "                        \"token\": serialize_node(operator),\n",
    "                    }\n",
    "                ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "            }\n",
    "        if operator.type in (\"/=\", \"%=\",):\n",
    "            return {\n",
    "                \"kind\": \"divide by zero\",\n",
    "                \"parent\": serialize_node(n),\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"kind\": \"operator\",\n",
    "                        \"token\": serialize_node(operator),\n",
    "                    }\n",
    "                ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "            }\n",
    "    # binary operator expression\n",
    "    elif n.type in (\"update_expression\",):\n",
    "        operator = get_child(n, lambda c: c.type in (\"++\", \"--\"))\n",
    "        assert not operator.is_named, (\"update\", n, n.children)\n",
    "\n",
    "        return {\n",
    "            \"kind\": \"overflow/underflow\",\n",
    "            \"parent\": serialize_node(n),\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"kind\": \"operator\",\n",
    "                    \"token\": serialize_node(operator),\n",
    "                }\n",
    "            ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "        }\n",
    "    elif n.type in (\"binary_expression\",):\n",
    "        operator = get_child_at_index(n, 1)\n",
    "        assert not operator.is_named, (\"binary_operator\", n, n.children)\n",
    "        if operator.type in (\"+\", \"-\", \"*\",):\n",
    "            return {\n",
    "                \"kind\": \"overflow/underflow\",\n",
    "                \"parent\": serialize_node(n),\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"kind\": \"operator\",\n",
    "                        \"token\": serialize_node(operator),\n",
    "                    }\n",
    "                ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "            }\n",
    "        if operator.type in (\"/\", \"%\",):\n",
    "            return {\n",
    "                \"kind\": \"divide by zero\",\n",
    "                \"parent\": serialize_node(n),\n",
    "                \"tokens\": [\n",
    "                    {\n",
    "                        \"kind\": \"operator\",\n",
    "                        \"token\": serialize_node(operator),\n",
    "                    }\n",
    "                ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "            }\n",
    "    # array/pointer operator expression (*, ->, [])\n",
    "    elif n.type in (\"pointer_expression\"):\n",
    "        operator = get_child_at_index(n, 0)\n",
    "        if operator.type in (\"pointer_expression\", \"&\"):\n",
    "            return\n",
    "        assert operator.type == \"*\", (\"*\", n, n.children)\n",
    "\n",
    "        return {\n",
    "            \"kind\": \"dereference\",\n",
    "            \"parent\": serialize_node(n),\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"kind\": \"star\",\n",
    "                    \"token\": serialize_node(operator),\n",
    "                }\n",
    "            ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "        }\n",
    "    elif n.type in (\"field_expression\"):\n",
    "        operator = get_child_at_index(n, 1)\n",
    "        if operator.type in (\".\",):\n",
    "            return\n",
    "        assert operator.type == \"->\", (\"->\", n, n.children)\n",
    "\n",
    "        return {\n",
    "            \"kind\": \"field\",\n",
    "            \"parent\": serialize_node(n),\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"kind\": \"arrow\",\n",
    "                    \"token\": serialize_node(operator),\n",
    "                }\n",
    "            ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "        }\n",
    "    elif n.type in (\"subscript_expression\"):\n",
    "        operator1 = get_child_at_index(n, 1)\n",
    "        operator2 = get_child_at_index(n, 3)\n",
    "        assert operator1.type == \"[\" and operator2.type == \"]\", (\"index\", n, n.children)\n",
    "\n",
    "        return {\n",
    "            \"kind\": \"index\",\n",
    "            \"parent\": serialize_node(n),\n",
    "            \"tokens\": [\n",
    "                {\n",
    "                    \"kind\": \"lbracket\",\n",
    "                    \"token\": serialize_node(operator1),\n",
    "                },\n",
    "                {\n",
    "                    \"kind\": \"rbracket\",\n",
    "                    \"token\": serialize_node(operator2),\n",
    "                }\n",
    "            ] if abstract else [{\"kind\": \"child\", \"token\": serialize_node(c)} for c in dfs(n)]\n",
    "        }\n",
    "        \n",
    "\n",
    "parser = get_parser(\"c\")\n",
    "def parse(code):\n",
    "    if isinstance(code, str):\n",
    "        code = code.encode()\n",
    "    tree = parser.parse(code)\n",
    "    return tree\n",
    "\n",
    "def dfs(node):\n",
    "    stack = [node]\n",
    "    while stack:\n",
    "        current_node = stack.pop()\n",
    "        yield current_node\n",
    "        stack.extend(current_node.children)\n",
    "\n",
    "def walk(tree, abstract=False, enter_compound=True):\n",
    "    if isinstance(tree, tree_sitter.Tree):\n",
    "        root = tree.root_node\n",
    "    else:\n",
    "        root = tree\n",
    "    marked_tokens = []\n",
    "    q = [(root, 0)]\n",
    "    while len(q) > 0:\n",
    "        n, indent = q.pop()\n",
    "        try:\n",
    "            mt = mark_tokens(n, abstract=abstract)\n",
    "        except AssertionError as e:\n",
    "            if not any(c.has_error for c in n.children):\n",
    "                print(\"***AssertionError***\", e)\n",
    "            mt = None\n",
    "        if mt is not None:\n",
    "            # print(\"***Marked token***\", mt)\n",
    "            marked_tokens.append(mt)\n",
    "        if n.type not in (\"string_literal\", \"char_literal\"):\n",
    "            if n.type == \"compound_statement\" and not enter_compound:\n",
    "                continue\n",
    "            next_indent = [indent+1] * len(n.children)\n",
    "            q.extend(reversed(list(zip(n.children, next_indent))))\n",
    "    return marked_tokens\n",
    "\n",
    "\n",
    "def to_marked_ids(pvs):\n",
    "    for m in pvs:\n",
    "        for tok in m[\"tokens\"]:\n",
    "            yield tok[\"token\"][\"id\"]\n",
    "\n",
    "def get_pvs_v2(code, abstract=False):\n",
    "    if isinstance(code, str):\n",
    "        code = code.encode()\n",
    "    tree = parser.parse(code)\n",
    "    # print_tree(tree.root_node)\n",
    "    ns = list(dfs(tree.root_node))\n",
    "    pvs = walk(tree, abstract=abstract, enter_compound=True)\n",
    "    marked_ids = set(to_marked_ids(pvs))\n",
    "    pvs_map = [1 if n.id in marked_ids else 0 for n in ns]\n",
    "    ast_tokens = [n.text.decode() for n in ns]\n",
    "    return ast_tokens, pvs_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"int main(int argc, char **argv)\n",
    "{\n",
    "    int x = 10 + 100;       // immediate expression - easy to filter out\n",
    "    int y = 10 + x;         // not immediate but known provenance\n",
    "    int z = 10 + y + argc;  // unknown provenance\n",
    "    char *s = (char *)malloc(z + y + x);\n",
    "    s[10] = 'a';\n",
    "    int result = (int)s[10];\n",
    "    printf(\"%c\\n\", *s);\n",
    "    free(s);\n",
    "    free(s);\n",
    "    s = 0;\n",
    "    s += 1;\n",
    "    s++;\n",
    "    struct foo* ss;\n",
    "    ss->bar = x;\n",
    "    *ss.baz = y;\n",
    "    return result;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ast_tokens, pvs_map=get_pvs_v2(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=\"\"\"int main()\n",
    "        {\n",
    "            int a;\n",
    "            printf(\"hello\")\n",
    "            scanf(\"%d\",&a);\n",
    "            printf(a);\n",
    "        }\n",
    "    \"\"\"\n",
    "ast_tokens, pvs_map=get_pvs_v2(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(len(ast_tokens))\n",
    "print(pvs_map.count(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
